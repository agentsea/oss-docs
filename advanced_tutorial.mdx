---
title: (WIP) Building Your Agent from Scratch
description: A detailed guide from zero to GUI surfer with multiple approaches
icon: screwdriver-wrench
---

In this guide, we'll show how to create your own agent using surfkit and some of the techniques we came up along the way. 

## Prerequisites

* Install `poetry` (see [Poetry docs](https://python-poetry.org/docs/)).
* Install `surfkit` (see [Quickstart](./quickstart)).
* Set up your local or cloud environment (see [Configuration](./configuration)).
* Install Tesseract on your machine (see [Tesseract docs](https://tesseract-ocr.github.io/tessdoc/Installation.html)).

## Creating an Agent

Creating a dummy agent that follows the SurfKit protocol is super easy:

```
mkdir surfhamster
cd surfhamster 
surfkit new
```

The last command will ask you to answer a few questions:

```
Enter agent name: SurfHamster
Describe the agent: The AI agent that can navigate GUIs and do tasks in them.
Enter git user reference (Your Name <your.email@gmail.com>): 
Enter docker image repo: 
Enter icon url (https://tinyurl.com/y5u4u7te): 
```

Feel free to leave the docker image repo empty for now and use a standard icon. When you run these commands, an Agent project will be initialized inside the folder that you created. It contains all components you need to give it a try:

```
surfkit create tracker -n hound001 -r docker 
surfkit create device -n eve001 -p qemu
surfkit create agent -n robby001 -r process

surfkit solve -a robby001 --device eve001 --tracker hound001 \
    -d "Find a cool cat image in the internet" --starting-url "google.com"
```

If the browser tab with a VM desktop and an agent log opens and the agent starts solving the task, congratulations: you have just created your first agent!

## How does it work?

### Code

Let's briefly look inside the repository. There is a bunch of files there, but the most critical are the following:

* `agent.yaml` is a configuration file; it contains a few self-explanatory sections: 
  * you will need to change the docker image repo later when you are ready to publish it; 
  * you may also want to change the icon; 
  * by default, your agent will run locally; 
  * this agent is designed to work with a “desktop” device as other GUI-driven agents that we build;
* `server.py` is just an utility class used to host the server with the agent process;
* `agent.py` is the main class that implements the logic of the agent: 
  * at the beginning of the task execution we explain to the LLM what this agent is expected to do; 
  * then we enter the loop: 
    * given the task and the history of the chat, as well as the state of the desktop, we ask an LLM to give us the next action and the reason for it; 
    * the next action is returned by LLM as a JSON, which is checked against the available device and executed upon it;
  * we exit the loop when either the LLM returns an action marked as "result" (which means that it thinks that the task is solved), or the maximum amount of iterations is reached (30 by default).

### Architecture

This type of agents is designed to work with a Desktop device. The Desktop device has an interface that allows manipulating the VM (in the cloud or a local one) via a mouse and a keyboard programmatically. You have just created it above via `surfkit create device`.

When the agent is trying to solve a given task, it manipulates the device. The device implements the `Tool` interface and therefore has a schema of actions. We get this schema in JSON format, ask an LLM to return the next action in this format, and then on each step of solving a task, we have an action that can be passed back to the device to be executed. 

For example, the action returned by LLM, may look like this:

```json
{
  "observation": "The mouse cursor is positioned inside the Google search bar.",
  "reason": "To find a cat image, we need to type the search query 'cool cat images' into the search bar.",
  "action": {
    "name": "type_text",
    "parameters": {
      "text": "cool cat images"
    }
  }
}
```

When this object is returned to device, the device can execute it: that is, type the text "cool cat images" using keyboard.

As said above, the actions include the low-level operations with a mouse and a keyboard, like moving mouse, clicking on coordinates, typing letters. They also include taking the screenshot and getting the current mouse coordinates, which helps an MLLM to choose the next action towards the task completion.

The agent we just created works using these primitives: typing text and clicking mouse. Cool, right? However, there is a problem.

### Problem

And this is a serious problem: at the moment of writing this tutorial (June 2024, "gpt-4o" was released not so long ago), all frontier MLLMs are horrible att identifying coordinates of the object on a screenshot. They can reason quite well about what should be clicked or typed to achieve the goal, but they can't return correct coordinates for that.

Therefore, the main area of improvements for this type of agents is helping them to **convert an idea on what should be clicked to actual coordinates on a screen**. To do that, we need to expand the toolset of our device and add a semantic layer to it, so that instead of "click on (400, 350)" our agent will advice to "click on a big 'Search' button at the bottom of the screen".

From the code point of view, we'll do the following:

* We'll introduce the `SemanticDesktop` class, which is essentially a wrapper around the Desktop that we already have, and inherits all the actions that it provides.
* We'll then update the `Agent` class to use the `SemanticDesktop` alongside the actual `Desktop`: we want the actions to be taken from and by `SemanticDesktop` and translated to the low-level `Desktop` operations when needed; we also want to keep using the screenshoting and mouse-catching abilities from `Desktop`;
* After that, we'll introduce a new action to the `SemanticDesktop`: `click_object`.

## SemanticDesktop

*Note*: You can find the code for this step of the tutorial [here](https://github.com/agentsea/surfhamster/tree/1fc5562be20c2731974222405160aef2f4168717).

First, we need to refine our `SemanticDesktop`. It will interit from `Tool`, which would allow us pass it to the MLLM.

```
code for tool.py
```

The most interesting part of this class is that we add a new method, `click_object`:

```python
    @action
    def click_object(self, description: str, type: str) -> None:
        """Click on an object on the screen

        Args:
            description (str): The description of the object including its general location, for example
                "a round dark blue icon with the text 'Home' in the top-right of the image", please be a generic as possible
            type (str): Type of click, can be 'single' for a single click or
                'double' for a double click. If you need to launch an application from the desktop choose 'double'
        """
        info = self.desktop.info()
        screen_size = info["screen_size"]
        self._click_coords(screen_size["x"] // 2, screen_size["y"] // 2, "single")
```

As you can see, right now it is quite non-intelligent: it simply returns the coordinated at middle of the screen. Don't worry, we'll work on it later!

As we now have this class, we can update the `Agent` class too:

```
code for agent.py
```

Note that we replace some of the usages of the `Desktop` device by the `SemanticDesktop` device, but not all of them. The best way to explain it is that we get observations from the `Desktop` (a screenshot and mouse coordinates), but we run the actions of and by the `SemanticDesktop`. We also remove some actions we don't need our agent to know:

```python
        tools = semdesk.json_schema(
            exclude_names=[
                "move_mouse",
                "click",
                "drag_mouse",
                "mouse_coordinates",
                "take_screenshot",
                "open_url",
                "double_click",
            ]
        )
```

If you run the agent now, you'll notice in the console logs, that the schema (the available actions) is changed, and that the agent can now return a new kind of actions:

```python
{
  "observation": "The cursor is near the address bar and the search tab at the top left of the Google homepage.",
  "reason": "To search for a cool cat image, the next step is to type the search query into the Google search box located in the center of the screen.",
  "action": {
    "name": "click_object",
    "parameters": {
      "description": "the Google search box in the center of the screen",
      "type": "single"
    }
  }
}
```

The only problem now is that the implementation of this `click_object` function is pretty dumb. Let's fix this.

## Adding the Grid

*Note*: You can find the code for this step of the tutorial [here](https://github.com/agentsea/surfhamster/tree/4a80d5f1fab16d61a0669ffca1507f6768c2b8dd).

There are many ways to assist an MLLM in picking correctly the exact location of the object on a screenshot. None of them is perfect (to the best of our knowledge at the moment of writing this tutorial), but combining a few in one agent can guarantee pretty high accuracy. Let's start with something simple.

We call this approach "The Grid". The idea is to put a bunch of dots with numbers in the corners of the cells on NxN grid on a screen. Honestly, it's easier to show than to explain:

**IMAGE WITH GRID**

If we desaturate the original screenshot and put this grid on top, we may ask an MLLM which dot is the closest one to the place which the agent wants to click (for example, a search bar or a button). In order to do that, we'll start a tiny thread with an MLLM (outside of the main thread) just to address this question. 

We then simply convert the number that an MLLM returns back to the coordinates on a screen.

First, we need to define a bunch of utility functions to generate this grid, merge it with the main image, and also convert images to and from b64 because it's the only format of the image that gpt-4o accepts.

```
code for image.py
```

Now, we can seriously update the `click_object` function:

```
code for click_object function in tool.py
```

When you run the agent now, you can see the images with grid that it generates, in the debug tab. You can see that MLLM is picking the correct number pretty reliably. This method is obviously more intelligent than pciking the middle of the screen. However, there is a good change to miss the correct spot because the element we are interested it is not under the middle of the dot.

To address this issue, we go on Zooming in and scaling up the part of the screenshot surrounding the chosen dot. You can see the implementation in the [SurfSlicer](https://github.com/agentsea/surfslicer) agent.












