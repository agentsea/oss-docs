---
title: (WIP) Building Your Agent from Scratch
description: A detailed guide from zero to GUI surfer with multiple approaches
icon: screwdriver-wrench
---

In this guide, we'll show how to create your own agent using surfkit and some of the techniques we came up along the way. 

## Prerequisites

* Install `poetry` (see [Poetry docs](https://python-poetry.org/docs/)).
* Install `surfkit` (see [Quickstart](./quickstart)).
* Set up your local or cloud environment (see [Configuration](./configuration)).
* Install Tesseract on your machine (see [Tesseract docs](https://tesseract-ocr.github.io/tessdoc/Installation.html)).

## Creating an Agent

Creating a dummy agent that follows the SurfKit protocol is super easy:

```
mkdir surfhamster
cd surfhamster 
surfkit new
```

The last command will ask you to answer a few questions:

```
Enter agent name: SurfHamster
Describe the agent: The AI agent that can navigate GUIs and do tasks in them.
Enter git user reference (Your Name <your.email@gmail.com>): 
Enter docker image repo: 
Enter icon url (https://tinyurl.com/y5u4u7te): 
```

Feel free to leave the docker image repo empty for now and use a standard icon. When you run these commands, an Agent project will be initialized inside the folder that you created. It contains all components you need to give it a try:

```
surfkit create tracker -n hound001 -r docker 
surfkit create device -n eve001 -p qemu
surfkit create agent -n robby001 -r process

surfkit solve -a robby001 --device eve001 --tracker hound001 \
    -d "Find a cool cat image in the internet" --starting-url "google.com"
```

If the browser tab with a VM desktop and an agent log opens and the agent starts solving the task, congratulations: you have just created your first agent!

## How does it work?

### Code

Let's briefly look inside the repository. There is a bunch of files there, but the most critical are the following:

* `agent.yaml` is a configuration file; it contains a few self-explanatory sections: 
  * you will need to change the docker image repo later when you are ready to publish it; 
  * you may also want to change the icon; 
  * by default, your agent will run locally; 
  * this agent is designed to work with a “desktop” device as other GUI-driven agents that we build;
* `server.py` is just an utility class used to host the server with the agent process;
* `agent.py` is the main class that implements the logic of the agent: 
  * at the beginning of the task execution we explain to the LLM what this agent is expected to do; 
  * then we enter the loop: 
    * given the task and the history of the chat, as well as the state of the desktop, we ask an LLM to give us the next action and the reason for it; 
    * the next action is returned by LLM as a JSON, which is checked against the available device and executed upon it;
  * we exit the loop when either the LLM returns an action marked as "result" (which means that it thinks that the task is solved), or the maximum amount of iterations is reached (30 by default).

### Architecture

This type of agents is designed to work with a Desktop device. The Desktop device has an interface that allows manipulating the VM (in the cloud or a local one) via a mouse and a keyboard programmatically. You have just created it above via `surfkit create device`.

When the agent is trying to solve a given task, it manipulates the device. The device implements the `Tool` interface and therefore has a schema of actions. We get this schema in JSON format, ask an LLM to return the next action in this format, and then on each step of solving a task, we have an action that can be passed back to the device to be executed. 

For example, the action returned by LLM, may look like this:

```json
{
  "observation": "The mouse cursor is positioned inside the Google search bar.",
  "reason": "To find a cat image, we need to type the search query 'cool cat images' into the search bar.",
  "action": {
    "name": "type_text",
    "parameters": {
      "text": "cool cat images"
    }
  }
}
```

When this object is returned to device, the device can execute it: that is, type the text "cool cat images" using keyboard.

As said above, the actions include the low-level operations with a mouse and a keyboard, like moving mouse, clicking on coordinates, typing letters. They also include taking the screenshot and getting the current mouse coordinates, which helps an MLLM to choose the next action towards the task completion.

The agent we just created works using these primitives: typing text and clicking mouse. Cool, right? However, there is a problem.

### Problem

And this is a serious problem: at the moment of writing this tutorial (June 2024, "gpt-4o" was released not so long ago), all frontier MLLMs are horrible att identifying coordinates of the object on a screenshot. They can reason quite well about what should be clicked or typed to achieve the goal, but they can't return correct coordinates for that.

Therefore, the main area of improvements for this type of agents is helping them to **convert an idea on what should be clicked to actual coordinates on a screen**. To do that, we need to expand the toolset of our device and add a semantic layer to it, so that instead of "click on (400, 350)" our agent will advice to "click on a big 'Search' button at the bottom of the screen".

From the code point of view, we'll do the following:

* We'll introduce the `SemanticDesktop` class, which is essentially a wrapper around the Desktop that we already have, and inherits all the actions that it provides.
* We'll then update the `Agent` class to use the `SemanticDesktop` alongside the actual `Desktop`: we want the actions to be taken from and by `SemanticDesktop` and translated to the low-level `Desktop` operations when needed; we also want to keep using the screenshoting and mouse-catching abilities from `Desktop`;
* After that, we'll introduce a new action to the `SemanticDesktop`: `click_object`.










